{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-20 01:50:52 __init__.py:190] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM\n",
    "from vllm.sampling_params import SamplingParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"mistral_models/7B-Instruct-v0.3\"\n",
    "\n",
    "sampling_params = SamplingParams(max_tokens=8192)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-20 01:50:56 config.py:2382] Downcasting torch.float32 to torch.float16.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-20 01:51:03 config.py:542] This model supports multiple tasks: {'classify', 'score', 'embed', 'generate', 'reward'}. Defaulting to 'generate'.\n",
      "INFO 02-20 01:51:03 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.2) with config: model='mistral_models/7B-Instruct-v0.3', speculative_config=None, tokenizer='mistral_models/7B-Instruct-v0.3', skip_tokenizer_init=False, tokenizer_mode=mistral, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.MISTRAL, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=mistral_models/7B-Instruct-v0.3, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "WARNING 02-20 01:51:04 interface.py:284] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.\n",
      "INFO 02-20 01:51:04 cuda.py:230] Using Flash Attention backend.\n",
      "INFO 02-20 01:51:05 model_runner.py:1110] Starting to load model mistral_models/7B-Instruct-v0.3...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e66ab16f5bb0433bb49eb3b917b830ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-20 01:51:29 model_runner.py:1115] Loading model weights took 5.5159 GB\n",
      "INFO 02-20 01:51:36 worker.py:267] Memory profiling takes 6.68 seconds\n",
      "INFO 02-20 01:51:36 worker.py:267] the current vLLM instance can use total_gpu_memory (8.00GiB) x gpu_memory_utilization (0.90) = 7.20GiB\n",
      "INFO 02-20 01:51:36 worker.py:267] model weights take 5.52GiB; non_torch_memory takes 0.03GiB; PyTorch activation peak memory takes 0.63GiB; the rest of the memory reserved for KV Cache is 1.03GiB.\n",
      "INFO 02-20 01:51:36 executor_base.py:110] # CUDA blocks: 526, # CPU blocks: 2048\n",
      "INFO 02-20 01:51:36 executor_base.py:115] Maximum concurrency for 2048 tokens per request: 4.11x\n",
      "INFO 02-20 01:51:38 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 35/35 [02:48<00:00,  4.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-20 01:54:26 model_runner.py:1562] Graph capturing finished in 168 secs, took 0.29 GiB\n",
      "INFO 02-20 01:54:26 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 177.64 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "llm = LLM(\n",
    "    model=model_name, \n",
    "    tokenizer_mode=\"mistral\", \n",
    "    config_format=\"mistral\", \n",
    "    load_format=\"mistral\",\n",
    "    cpu_offload_gb=8,\n",
    "    max_model_len=2048\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-20 01:59:52 chat_utils.py:332] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.\n",
      "WARNING 02-20 01:59:52 chat_utils.py:996] 'add_generation_prompt' is not supported for mistral tokenizer, so it will be ignored.\n",
      "WARNING 02-20 01:59:52 chat_utils.py:1000] 'continue_final_message' is not supported for mistral tokenizer, so it will be ignored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:36<00:00, 36.95s/it, est. speed input: 0.65 toks/s, output: 1.08 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " No, the sum of 1 + 1 is a basic arithmetic operation that can be easily understood and answered quickly, usually without any need for thought. The answer is 2.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Do we need to think for 10 seconds to find the answer of 1 + 1?\"\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt\n",
    "    },\n",
    "]\n",
    "\n",
    "outputs = llm.chat(messages, sampling_params=sampling_params)\n",
    "\n",
    "print(outputs[0].outputs[0].text)\n",
    "# You don't need to think for 10 seconds to find the answer to 1 + 1. The answer is 2,\n",
    "# and you can easily add these two numbers in your mind very quickly without any delay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "Processed prompts: 100%|██████████| 2/2 [00:50<00:00, 25.33s/it, est. speed input: 0.83 toks/s, output: 13.66 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " To implement a RAG (Red, Amber, Green) system and fine-tuning with a machine learning model, you can follow the steps below:\n",
      "\n",
      "1. Define your RAG criteria:\n",
      "- Red: High-risk, critical issues, or severe errors\n",
      "- Amber: Medium-risk issues, warnings, or potential issues\n",
      "- Green: Low-risk, normal conditions, or no issues\n",
      "\n",
      "2. Collect and label your data:\n",
      "- Gather a dataset of instances that your model will process. Examples could be text snippets, images, or any other type of input your model supports.\n",
      "- Label the data according to the RAG criteria based on the output or possible outcomes of your model.\n",
      "\n",
      "Typically, you may find it challenging to obtain fully labeled datasets for the RAG system, as it requires human expertise to evaluate the model's predictions and determine the correct RAG classification. If data is scarce, consider using active learning to efficiently gather the most informative examples for model training.\n",
      "\n",
      "3. Preprocess data and prepare the model:\n",
      "- Preprocess the labeled data by normalizing, cleaning, and augmenting it as needed for better model performance.\n",
      "- Choose a pre-trained model or design a new model tailored to your specific task. Common choices include deep neural networks, support vector machines, or decision trees, depending on the type of data you're working with.\n",
      "\n",
      "4. Fine-tune the model:\n",
      "- Split your labeled dataset into training, validation, and test sets (e.g. 80/10/10 split).\n",
      "- Train the model using the training data, adjusting the learning rate, optimization algorithm, and coefficients to minimize errors.\n",
      "- Evaluate the model's performance using the validation set to avoid overfitting.\n",
      "- Fine-tune the model hyperparameters, such as the learning rate, batch size, and number of epochs, until you achieve satisfactory results.\n",
      "\n",
      "5. Integrate the RAG system:\n",
      "- Modify the model's output layer to produce three classes for RAG classification (Red, Amber, Green). You can use multiple output layers for multiclass classification or a sigmoid approach with binary output layers for one-vs-one or one-vs-all classification.\n",
      "- Tune the threshold values for each RAG class to optimize the balance when transferring risk levels (i.e., correctly identify Red, Amber, and Green instances across the various risk levels).\n",
      "- After fine-tuning the RAG system, save the updated model and integrate it into your application.\n",
      "\n",
      "6. Continuous monitoring and improvement:\n",
      "- Monitor the model's performance in real-world conditions to ensure it continues to function as intended.\n",
      "- When new data becomes available, retrain the model to maintain its accuracy and stay up-to-date with any changes.\n",
      "- Continuously gather feedback and iterate on the RAG system to improve its efficiency and effectiveness.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Do we need to think for 10 seconds to find the answer of 1 + 1?\"\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt\n",
    "    },\n",
    "]\n",
    "\n",
    "outputs = llm.chat(messages, sampling_params=sampling_params)\n",
    "\n",
    "print(outputs[0].outputs[0].text)\n",
    "# You don't need to think for 10 seconds to find the answer to 1 + 1. The answer is 2,\n",
    "# and you can easily add these two numbers in your mind very quickly without any delay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "Processed prompts: 100%|██████████| 1/1 [03:00<00:00, 180.35s/it, est. speed input: 0.13 toks/s, output: 1.14 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " No, you don't need to think for 10 seconds to calculate the cube of 69. You can do it mentally or using a calculator. However, it's a large number and might take a few moments. The answer is 25,967,629,629. You can verify it by calculating it step by step:\n",
      "\n",
      "69 (multiplied by itself) = 4721\n",
      "4721 (multiplied by itself) = 22,092,091\n",
      "22,092,091 (multiplied by 69) = 1,487,913,189\n",
      "\n",
      "Or you can use a calculator for the exponentiation right away: 69^3 ≈ 25,967,629,629.00 (when rounded to 13 significant figures).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Do we need to think for 10 seconds to find the answer of 69^3?\"\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt\n",
    "    },\n",
    "]\n",
    "\n",
    "outputs = llm.chat(messages, sampling_params=sampling_params)\n",
    "\n",
    "print(outputs[0].outputs[0].text)\n",
    "# You don't need to think for 10 seconds to find the answer to 1 + 1. The answer is 2,\n",
    "# and you can easily add these two numbers in your mind very quickly without any delay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
